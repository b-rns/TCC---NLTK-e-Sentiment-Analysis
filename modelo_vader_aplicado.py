# -*- coding: utf-8 -*-
"""modelo_vader_aplicado.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5AavuyQQvDiKWs3Uxes-u85DIaXTnXF

**A percepção dos usuários de redes sociais sobre a tecnologia de NFT e suas aplicações analisadas sob a ótica da análise de sentimentos com processamento de linguagem natural**

PARTE 3: Aplicação do dicionário léxico VADER (Valence Aware Dictionary for Sentiment Reasoning) em um modelo de análise de sentimentos/emoções para a avaliação de tweets

No campo de estudo da Análise de Conteúdo em textos, a análise de sentimentos representa um método para a realização de uma Analise Relacional de conteúdo, em que o foco da pesquisa é extrair sentido não somente da presença dos conceitos, mas também da relação entre esses conceitos no texto, e pode ser definida como “Um método que busca compreender a emoção dos conceitos presentes no texto, geralmente através de análises temáticas e de elementos relacionados a essas emoções” (SILVA, 2018,p.15)

## Importações e referências

https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664#:~:text=VADER%20(%20Valence%20Aware%20Dictionary%20for,directly%20to%20unlabeled%20text%20data.



https://dgarcia-eu.github.io/SocialDataScience/3_Affect/032_UnsupervisedSentimentAnalysis/UnsupervisedSentimentAnalysis.html



https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483
"""

# Analise de dados
import pandas as pd
import numpy as np

# bibliotecas para manipulação de texto
import re, string

# VADER
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Exportação em Excel
!pip install xlsxwriter

# Explorações gráficas
import matplotlib.pyplot as plt
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

# Para a contagem da Frequência de palavras
from nltk import FreqDist

df = pd.read_excel('/content/opiniao_modelo.xlsx', index_col=0)
df_avaliacao = pd.read_excel('/content/avaliacao_modelo.xlsx')

# Visualizando o dataset de avaliação do modelo
df_avaliacao.head(3)

# Visualizando o dataset principal com os dados rotulados
df.head(3)

df_avaliacao.texto_limpo[1]

df_avaliacao.texto_original[1]

# Tamanho do dataset completo 
len(df)

# Tamanho do dataset de avaliacao
len(df_avaliacao)

'''
texto_unido = []
for c in df_avaliacao.texto:
  texto_unido.append("".join(s for s in c).replace("'","").replace("[","").replace(",","").replace("]","").replace("discord",""))
df_avaliacao['texto'] = texto_unido
df_avaliacao.head()
'''

# Removendo os números para melhorar o processamento do VADER
texto_unido = []
for c in df.texto:
  a = re.sub(r'[0-9]+', '', str(c)).replace('"',"").replace(".","")
  texto_unido.append(a)
df['texto'] = texto_unido
df.head()

"""## VADER

O VADER (Valence Aware Dictionary for Sentiment Reasoning) é um dicionário léxico criado com uma combinação de métodos quantitativos e qualitativos desenvolvido e testado em diferentes contextos, mas com desempenho especialmente melhor em textos no formato de micro-blog, tal como postagens de redes sociais como o Twiter (Hutto & Gilbert, 2014).

### Criando e testando o modelo
"""

sid = SentimentIntensityAnalyzer()
df_vader_av = df_avaliacao.copy()

# Aplicando sobre o texto original:
df_vader_av['scores'] = df_vader_av['texto_limpo'].apply( lambda texto: sid.polarity_scores(texto)) 
df_vader_av.head()

df_vader_av['compound'] = df_vader_av['scores'].apply( lambda score_dict: score_dict['compound']) 
df_vader_av.head(3)

# positive = 1 / negative = -1 / neutral = 0
df_vader_av['comp_score'] = df_vader_av['compound'].apply(lambda c:1 if c > 0.4 else -1 if c < -0.4 else 0)
df_vader_av.tail(3)

df_vader_av.comp_score.describe()

# Todos os dados foram classificados como positivo, negativo ou neutro
df_vader_av.comp_score.unique()

df_vader_av['vader'] = df_vader_av['comp_score']
df_vader_av.head()

cm = pd.crosstab(df_vader_av.avaliacao, df_vader_av.vader, rownames=['real'], colnames=['predito'])
cm

# Acuracidade
print(f'Acurácia: {round((cm[0][0] + cm[1][1] + cm[-1][-1])/len(df_vader_av),2)*100}%')

# Detecção de neutros
print(f'Detecção de Neutros: {round(cm[0][0] / cm.sum(axis = 1)[0],2) * 100} %')

# Detecção de positivos
print(f'Detecção de Positivos: {round(cm[1][1] / cm.sum(axis = 1)[1], 2) * 100}%')

#Detecção de negativos
print(f'Detecção de Negativos: {round(cm[-1][-1] / cm.sum(axis = 1)[-1],2)*100}%')

"""### Aplicando o modelo sobre o dados"""

# Filtrando apenas os dados sem spam e definindo o dataset e a função de classificação de sentimentos
df = df[df['modelo'] == 'opinion']
sid = SentimentIntensityAnalyzer()
df_vader = df.copy()
df.tail()

# Aplicando o modelo
df_vader['scores'] = df_vader['texto'].apply( lambda texto: sid.polarity_scores(texto))
df_vader['compound'] = df_vader['scores'].apply( lambda score_dict: score_dict['compound']) 
df_vader['comp_score'] = df_vader['compound'].apply(lambda c:'positive' if c > 0.4 else 'negative' if c < -0.4 else 'neutral')
df_vader.head()

df_vader.comp_score.describe()

# Todos os dados foram classificados como positivo, negativo ou neutro
df_vader.comp_score.unique()

# Exportando o novo dataset com os dados classificados:
writer= pd.ExcelWriter('modelo_vader.xlsx', engine= 'xlsxwriter')
df_vader.to_excel(writer, sheet_name='vader')
writer.save()

"""## Explorando o Modelo"""

df_positive = df_vader[df_vader['comp_score']== 'positive']
df_negative = df_vader[df_vader['comp_score']== 'negative']
df_neutral  = df_vader[df_vader['comp_score']== 'neutral']

# Comparando o tamanho dos datasets
print(f'''Total de Tweets: {len(df_vader)}
Tweets positivos: {len(df_positive)} / ({round(len(df_positive)/len(df_vader)*100,2)}%)
Tweets negativos: {len(df_negative)} / ({round(len(df_negative)/len(df_vader)*100,2)}%)
Tweets neutros: {len(df_neutral)} / ({round(len(df_neutral)/len(df_vader)*100,2)}%)''')

"""### Exploração Visual: Tweets Positivos"""

texto_unido = ''
for c in df_positive.token:
  texto_unido += c
texto_unido = texto_unido.replace("'","").replace("[","").replace(",","").replace("]","")

lista_palavras = []
for tokens in list(texto_unido.split()):
  lista_palavras.append(tokens)

freq_dist_pos = FreqDist(lista_palavras)
tabela_frequencia = pd.DataFrame(list(freq_dist_pos.items()), columns = ["Word","Frequency"]).sort_values(by = "Frequency", ascending = False, ignore_index = True)
tabela_frequencia.head(30)

# Exportando o dataset com a frequência de palavras entre os textos positivos
writer= pd.ExcelWriter('frequencia_positivos.xlsx', engine= 'xlsxwriter')
tabela_frequencia.to_excel(writer, sheet_name='freq')
writer.save()

# Total de palavras do texto
len(lista_palavras)

print(f'''\n90% : {np.percentile(tabela_frequencia['Frequency'], 90)}\n
95% : {np.percentile(tabela_frequencia['Frequency'], 95)}\n
99% : {np.percentile(tabela_frequencia['Frequency'], 99).round()}\n
99,9% : {np.percentile(tabela_frequencia['Frequency'], 99.9).round()}''')

# As 50 palavras que mais apareceram representam menos de 1% (0.9996%) do total de palavras do corpus (e se repetem mais de 1434 vezes)
np.percentile(tabela_frequencia['Frequency'], (1 - 50/59578)*100)

(1-(1 - 50/56504))

# A distribuição da ocorrência das 50 palavras mais repetidas nos tweets

graf = tabela_frequencia[tabela_frequencia['Frequency']>414]
graf.plot(kind='bar', x='Word', y='Frequency', figsize=(15,7), legend=False)
plt.show

# gerando a núvem de palavras
wordcloud = WordCloud(background_color = "white", width = 1600, height= 800).generate(texto_unido)

# mostrando imagem
fig, ax = plt.subplots(figsize=(10,6))
ax.imshow(wordcloud, interpolation='bilinear')
ax.set_axis_off()

plt.imshow(wordcloud);
wordcloud.to_file("nft_limpo_wordcloud.png")

"""### Exploração Visual: Tweets Negativos"""

texto_unido = ''
for c in df_negative.token:
  texto_unido += c
texto_unido = texto_unido.replace("'","").replace("[","").replace(",","").replace("]","")

lista_palavras = []
for tokens in list(texto_unido.split()):
  lista_palavras.append(tokens)

freq_dist_pos = FreqDist(lista_palavras)
tabela_frequencia = pd.DataFrame(list(freq_dist_pos.items()), columns = ["Word","Frequency"]).sort_values(by = "Frequency", ascending = False, ignore_index = True)
tabela_frequencia.head(50)

# Exportando o dataset com a frequência de palavras entre os textos negativos
writer= pd.ExcelWriter('frequencia_negativos.xlsx', engine= 'xlsxwriter')
tabela_frequencia.to_excel(writer, sheet_name='freq')
writer.save()

tabela_frequencia[tabela_frequencia['Word']== 'fraud']

# Total de palavras do texto
len(lista_palavras)

print(f'''\n90% : {np.percentile(tabela_frequencia['Frequency'], 90)}\n
95% : {np.percentile(tabela_frequencia['Frequency'], 95)}\n
99% : {np.percentile(tabela_frequencia['Frequency'], 99)}\n
99,9% : {np.percentile(tabela_frequencia['Frequency'], 99.9).round()}''')

# As 50 palavras que mais apareceram representam menos de 1% (0.9996%) do total de palavras do corpus (e se repetem mais de 1434 vezes)
np.percentile(tabela_frequencia['Frequency'], (1 - 50/59578)*100)

(1-(1 - 50/56504))

# A distribuição da ocorrência das 50 palavras mais repetidas nos tweets

graf = tabela_frequencia[tabela_frequencia['Frequency']>97]
graf.plot(kind='bar', x='Word', y='Frequency', figsize=(15,7), legend=False)
plt.show

# gerando a núvem de palavras
wordcloud = WordCloud(background_color = "white", width = 1600, height= 800).generate(texto_unido)

# mostrando imagem
fig, ax = plt.subplots(figsize=(10,6))
ax.imshow(wordcloud, interpolation='bilinear')
ax.set_axis_off()

plt.imshow(wordcloud);
wordcloud.to_file("nft_limpo_wordcloud.png")

"""### Exploração Visual: Tweets Neutros"""

texto_unido = ''
for c in df_neutral.token:
  texto_unido += c
texto_unido = texto_unido.replace("'","").replace("[","").replace(",","").replace("]","")

lista_palavras = []
for tokens in list(texto_unido.split()):
  lista_palavras.append(tokens)

freq_dist_pos = FreqDist(lista_palavras)
tabela_frequencia = pd.DataFrame(list(freq_dist_pos.items()), columns = ["Word","Frequency"]).sort_values(by = "Frequency", ascending = False, ignore_index = True)
tabela_frequencia.head(50)

# Exportando o dataset com a frequência de palavras entre os textos neutros
writer= pd.ExcelWriter('frequencia_neutros.xlsx', engine= 'xlsxwriter')
tabela_frequencia.to_excel(writer, sheet_name='freq')
writer.save()

# Total de palavras do texto
len(lista_palavras)

print(f'''\n90% : {np.percentile(tabela_frequencia['Frequency'], 90)}\n
95% : {np.percentile(tabela_frequencia['Frequency'], 95)}\n
99% : {np.percentile(tabela_frequencia['Frequency'], 99)}\n
99,9% : {np.percentile(tabela_frequencia['Frequency'], 99.9).round()}''')

# As 50 palavras que mais apareceram representam menos de 1% (0.9996%) do total de palavras do corpus (e se repetem mais de 1434 vezes)
np.percentile(tabela_frequencia['Frequency'], (1 - 50/59578)*100)

(1-(1 - 50/56504))

# A distribuição da ocorrência das 50 palavras mais repetidas nos tweets

graf = tabela_frequencia[tabela_frequencia['Frequency']>197]
graf.plot(kind='bar', x='Word', y='Frequency', figsize=(15,7), legend=False)
plt.show

# gerando a núvem de palavras
wordcloud = WordCloud(background_color = "white", width = 1600, height= 800).generate(texto_unido)

# mostrando imagem
fig, ax = plt.subplots(figsize=(10,6))
ax.imshow(wordcloud, interpolation='bilinear')
ax.set_axis_off()

plt.imshow(wordcloud);
wordcloud.to_file("nft_limpo_wordcloud.png")